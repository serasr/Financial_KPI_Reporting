{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a564c2",
   "metadata": {},
   "source": [
    "# Financial KPI Reporting with PySpark & Power BI\n",
    "This notebook processes financial transaction data using **PySpark**, cleans and enriches it, and prepares a dataset for visualization in **Power BI**. It simulates missing amounts, calculates KPIs (cost, profit, margin), assigns risk segments, and outputs a clean CSV ready for business intelligence dashboards.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9996532",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Upload & Extraction](#2-data-upload--extraction)\n",
    "3. [Load Data](#3-load-data)\n",
    "4. [Join Datasets](#4-join-datasets)\n",
    "5. [Clean and Simulate Amount](#5-clean-and-simulate-amount)\n",
    "6. [Date & KPI Calculations](#6-date--kpi-calculations)\n",
    "7. [Risk Segmentation](#7-risk-segmentation)\n",
    "8. [Select Clean Columns](#8-select-clean-columns)\n",
    "9. [Export for Power BI](#9-export-for-power-bi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024a7e8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "### Install and configure Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Spark setup (run once per session) ---\n",
    "!apt-get install -qq openjdk-11-jdk-headless > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
    "!tar -xzf spark-3.4.1-bin-hadoop3.tgz\n",
    "!pip -q install findspark\n",
    "\n",
    "import os, findspark\n",
    "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"FinancialKPIReporting\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759f4e0",
   "metadata": {},
   "source": [
    "## 2. Data Upload & Extraction\n",
    "### Upload and extract Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff491c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Upload and extract your Kaggle zip ---\n",
    "from google.colab import files\n",
    "uploaded = files.upload()   # upload your \"archive (2).zip\"\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "zip_name = [k for k in uploaded.keys()][0]\n",
    "with zipfile.ZipFile(io.BytesIO(uploaded[zip_name]), \"r\") as z:\n",
    "    z.extractall(\"/content/transaction_data\")\n",
    "\n",
    "base = \"/content/transaction_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004b12d",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "### Load transactions, card details, and fraud labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ff575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Load data (NO MCC JOIN) ---\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_tx    = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{base}/transactions_data.csv\")\n",
    "df_cards = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{base}/cards_data.csv\")\n",
    "\n",
    "# Fraud labels are JSON dict: {transaction_id: \"Yes\"/\"No\"}\n",
    "import pandas as pd\n",
    "labels_pd = pd.read_json(f\"{base}/train_fraud_labels.json\").reset_index().rename(columns={\"index\":\"transaction_id\", \"target\":\"fraud\"})\n",
    "df_labels = spark.createDataFrame(labels_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae3af7",
   "metadata": {},
   "source": [
    "## 4. Join Datasets\n",
    "### Normalize IDs and join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Normalize ids to avoid join errors ---\n",
    "df_tx    = df_tx.withColumnRenamed(\"id\", \"transaction_id\")\n",
    "df_cards = df_cards.withColumnRenamed(\"id\", \"card_id\").withColumnRenamed(\"client_id\", \"card_client_id\")\n",
    "\n",
    "# --- 4) Join: transactions + labels + cards ---\n",
    "df = (df_tx\n",
    "      .join(df_labels, on=\"transaction_id\", how=\"left\")        # adds 'fraud' (Yes/No)\n",
    "      .fillna({\"fraud\": \"No\"})\n",
    "      .join(df_cards, on=\"card_id\", how=\"left\")                 # adds card attributes\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6911d2",
   "metadata": {},
   "source": [
    "## 5. Clean and Simulate Amount\n",
    "### Clean 'amount' field and simulate missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd38a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Clean amount and simulate where null ---\n",
    "from pyspark.sql.functions import regexp_replace, col, when, rand\n",
    "\n",
    "# If 'amount' looks like \"$1,234.50\", strip symbols then cast\n",
    "df = df.withColumn(\"amount\", regexp_replace(\"amount\", \"[$,]\", \"\"))\n",
    "df = df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "\n",
    "# Simulate amount for nulls (range ~50..3050, reproducible)\n",
    "df = df.withColumn(\n",
    "    \"amount\",\n",
    "    when(col(\"amount\").isNull(), (rand(seed=42) * F.lit(3000.0) + F.lit(50.0))).otherwise(col(\"amount\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793871f3",
   "metadata": {},
   "source": [
    "## 6. Date & KPI Calculations\n",
    "### Extract date parts and calculate KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6) Dates and derived fields ---\n",
    "from pyspark.sql.functions import to_timestamp, year, month\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"timestamp\", to_timestamp(\"date\"))    # auto-parse; adapt fmt if needed\n",
    "      .withColumn(\"year\",  year(\"timestamp\"))\n",
    "      .withColumn(\"month\", month(\"timestamp\"))\n",
    "     )\n",
    "\n",
    "# KPIs: cost/profit/margin (simple assumptions)\n",
    "df = (df\n",
    "      .withColumn(\"cost\",   col(\"amount\") * F.lit(0.70))\n",
    "      .withColumn(\"profit\", col(\"amount\") - col(\"cost\"))\n",
    "      .withColumn(\"profit_margin\",\n",
    "                  when(col(\"amount\") > 0, col(\"profit\")/col(\"amount\")).otherwise(F.lit(0.0)))\n",
    "      .withColumn(\"is_fraud\", when(col(\"fraud\") == \"Yes\", F.lit(1)).otherwise(F.lit(0)))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb72257",
   "metadata": {},
   "source": [
    "## 7. Risk Segmentation\n",
    "### Assign risk segments without UDF for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12963f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk segment (no UDF; faster)\n",
    "df = df.withColumn(\n",
    "    \"risk_segment\",\n",
    "    when(col(\"fraud\") == \"Yes\", \"High Risk\")\n",
    "    .when(col(\"amount\") > 1000, \"Medium Risk\")\n",
    "    .otherwise(\"Low Risk\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7adc2f",
   "metadata": {},
   "source": [
    "## 8. Select Clean Columns\n",
    "### Select relevant columns for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438304b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Select clean columns (no MCC) & avoid ambiguity ---\n",
    "wanted_cols = [\n",
    "    \"transaction_id\",\"date\",\"timestamp\",\"year\",\"month\",\n",
    "    \"client_id\",\"card_id\",\"card_client_id\",\n",
    "    \"amount\",\"cost\",\"profit\",\"profit_margin\",\n",
    "    \"use_chip\",\"merchant_id\",\"merchant_city\",\"merchant_state\",\"zip\",\n",
    "    \"fraud\",\"is_fraud\",\"risk_segment\",\n",
    "    \"card_brand\",\"card_type\",\"credit_limit\",\"card_on_dark_web\"\n",
    "]\n",
    "df_final = df.select(*[c for c in wanted_cols if c in df.columns])\n",
    "\n",
    "# Quick sanity checks\n",
    "df_final.printSchema()\n",
    "df_final.groupBy(\"risk_segment\").count().orderBy(F.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0894d0f",
   "metadata": {},
   "source": [
    "## 9. Export for Power BI\n",
    "### Save cleaned data to CSV and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6313f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8) Export for Power BI (single CSV inside a folder) ---\n",
    "out_dir = \"/content/cleaned_financial_data\"\n",
    "(df_final\n",
    " .coalesce(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(out_dir)\n",
    ")\n",
    "\n",
    "# Zip and download\n",
    "!zip -qr cleaned_financial_data.zip cleaned_financial_data\n",
    "from google.colab import files\n",
    "files.download(\"cleaned_financial_data.zip\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
